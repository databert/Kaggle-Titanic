---
title: "Titanic - Learning from Disaster with Random Forest and SVM"
author: "Berthold Jaeck"
date: "5/6/2017"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE, fig.width =6, fig.height=4)
```

# Loading data

The training and test data for this classification problem is loaded from the kaggle website.
```{r libs, echo=TRUE}
# Create the environment
library(dplyr)
library(ggplot2)
library(mice)
library(randomForest)
library(e1071)
```

```{r load}
# Loading the data
train<-tbl_df(read.csv("train.csv", header = TRUE, stringsAsFactors = F))
test<-tbl_df(read.csv("test.csv", header = TRUE, stringsAsFactors = F))
full<- bind_rows(train, test)

summary(full)
```
## Imputing missing data

Looking at the data summary we see that the *Age* variable still has a considerable amount of missing values. We could drop these observations, however, this would significantly reduce our data set by `r sum(is.na(full$Age))`. Hence, we wish to impute these values. One possibility to do that is by using the Multivariate Imputation by Chained Equations (MICE) library.
```{r mice imputation}
imp <- mice(full[, !names(full) %in% c('PassengerId','Name','Ticket','Cabin', 'Survived')], method='rf')

imp_data <- tbl_df(complete(imp))

par(mfrow=c(1,2))
hist(train$Age, freq=F, main='Age: data', 
  col='red', ylim=c(0,0.04))
hist(train$Age, freq=F, main='Age: imputed data', 
  col='blue', ylim=c(0,0.04))
```

As can be seen from our plot, the pattern of the imputed *Age* values fit the original data very well. For the following analysis, we will therefore replace the missing data values with the imputed data.
```{r}
full$Age<-imp_data$Age
full$Embarked<-imp_data$Embarked
full$Fare<-imp_data$Fare
```

## Feature engineering

### High fares save lives in first and second class

Looking at the survival as a function of the fare broken down into passenger class, we see that survival chance in 1st and 2nd class is higher when paying a higher fare. This can be also seen in detail when calculating the respective distibution medians.
```{r fare}
g<-ggplot(full, aes(Survived, Fare))+geom_point()+facet_grid(.~as.factor(full$Pclass))
print(g)
```

Hence it appears usfeful to create a VIP variable, that considers this relation.

```{r vipvar}
full %>% mutate(VIP=ifelse(((Pclass==1 & Fare>median(Fare[Pclass==1 & Survived==1], na.rm = TRUE)) | (Pclass==2 & Fare>median(Fare[Pclass==2 & Survived==1], na.rm = TRUE))), TRUE, FALSE))->full
```

Looking at the table *Survival* vs. *Fare* we can clearly see that paying more money saves your life:
```{r viptable}
table(full$VIP, full$Survived)
```

### Rich kids live longer

Looking at the *Survival* as a function of age broken down into *Pclass*, we see that it is always good to be a rich kid:

```{r richkidplot}
g<-ggplot(full, aes(Age, Survived))+geom_point()+facet_grid(.~as.factor(full$Pclass))
print(g)
```

While kids from 1st and 2nd class basically always survive if they are 15 years old or younger, it doesn't make much of a difference if you are more from the low income range. Hence, we'll create a variable to account for the rich kid factor:

```{r richkidvar}
full %>% mutate(RichKid=ifelse((Pclass==1 & Age<19) | (Pclass==2 & Age<15), TRUE, FALSE))->full
```

As we can see from the table,
```{r richkidtable}
table(full$RichKid, full$Survived)
```

it pays off to have wealthy parents that can afford a fancy ride on the Titanic.

### Family size

Speaking of rich kids, it may be interesting to see whether families had a survival bonus, when that ship started sinking. From an ethical point of view, a single man may die the honorable death in the cold sea whilst saving a young family or so, at least in the old times.
Anyway, let's have a look at the family size variables *Parch* and *SibSp*:

```{r famsizeplot}
g<-ggplot(full, aes(Parch+SibSp))+geom_histogram()+facet_grid(.~as.factor(full$Survived))
print(g)
```

As can be seen, being single may have given you a joyful time at the Titanic's bars, yet it is penalized regarding the survival chance. You want to travel with at least one but not more than two family companions in order to have a good survival chance. Let us put this observtion into a new variable *FamSize*, which we discretize to account for that particular size window.

```{r famsizevar}
full %>% mutate(FamSize=Parch+SibSp+1)->full
full$FamSizeD[full$FamSize == 1] <- 'single'
full$FamSizeD[full$FamSize < 5 & full$FamSize > 1] <- 'small'
full$FamSizeD[full$FamSize > 4] <- 'large'
```

Looking at the corresponding table,

```{r famsiztab}
table(full$FamSizeD, full$Survived)
```

it becomes obvious that both singles and members of larger families may have a hard time surviving.

# Building a model

Let us first convert all of our variables into factors such that they become suitable as predictive model input.

```{r converttofac}
fac<-c('PassengerId','Pclass','Sex','Embarked','FamSizeD', 'SibSp', 'Parch', 'RichKid', 'VIP')
full[fac] <- lapply(full[fac], function(x) as.factor(x))

```

Let us create some training and test data set's from the *train* data frame so that we can train and tune our model by using cross validation.

```{r }
train.ext <- full[1:891,]
test.ext <- full[892:1309,]

index <- 1:nrow(train.ext)
testindex <- sample(index, trunc(length(index)/3))
testset <- train.ext[testindex,]
trainset <- train.ext[-testindex,]
```    

## Random Forest Model

Let us start by building a prediction model that uses a randon forest algorithm, where the classification is based on a decision tree:
```{r}
rf.model<- randomForest(as.factor(Survived)~Pclass+Sex+SibSp+Age+Fare+Parch+Embarked+VIP+RichKid+FamSizeD, data=trainset)
```

Looking at the importance of the individual variables we see that *Sex*, *Age* and *Fare* are the most relevant categories upon which survival or death depends.

```{r}
importance<-importance(rf.model)
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = importance[ ,'MeanDecreaseGini'])
p<-ggplot(varImportance, aes(x = reorder(Variables, Importance), 
    y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + coord_flip()
print(p)
```

Now let us make use of our model and predict an outcome. We also make a confusion table to have a look on the quality of our model.

```{r}
prediction <- predict(rf.model, testset)
rf.tab<-table(pred=prediction, true = testset$Survived)
print(rf.tab)
```

That confusion table looks already fairly good.

## SVM model

Let's take a different route and usea SVM algorithm, which classifies by maximizing the decision boundary distance. Since number of *variables* n is relatively small on the order of 10 and number of observations is not very large on the order of 100, we want to use a radial kernel.

```{r}
svm.model<- svm(as.factor(Survived)~Pclass+Sex+SibSp+Age+Fare+Parch+Embarked+VIP+RichKid+FamSizeD, data=trainset, kernel="radial")
```

Let's predict some values and look at the confusion table
```{r}
svm.prediction <- predict(svm.model, testset[,-2])
svm.tab<-table(pred=svm.prediction, true = testset$Survived)
print(svm.tab)
```

As can be seen, our model still falls short compared to the random forest model. Let us run some cross validation test to enhance the quality of our model.
```{r}
tuned <- tune.svm(as.factor(Survived)~Pclass+Sex+SibSp+Age+Fare+Parch+Embarked+VIP+RichKid+FamSizeD, data = trainset, gamma = 10^(-6:-1), cost = 10^(1:2), kernel="radial")

summary(tuned)
```

Since we know now the ideal parameters, we can set up a tuned model and make an updated prediction:
```{r}
model.tuned<- svm(as.factor(Survived)~Pclass+Sex+SibSp+Age+Fare+Parch+Embarked+VIP+RichKid+FamSizeD, data=trainset, gamma=0.01, cost=100, kernel="radial")

prediction.tuned <- predict(model.tuned, testset[,-2])
tab.tuned<-table(pred=prediction.tuned, true = testset$Survived)
print(tab.tuned)
```
That looks already much better than the non-tuned model and gives similar quality of prediction as the random forest model.

# Predicting outcome and submit solution

Since the random forest model yields slightly better prediction that the SVM model, we will use it for the final prediction ont the test set.
```{r}
final.prediction<-predict(rf.model, test.ext)
```


```{r}
submission <- data.frame(PassengerID = test.ext$PassengerId, Survived = final.prediction)
write.csv(submission, file = 'titanic_solution.csv', row.names = F)
```

